{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c217550-ac39-4cd8-9772-8775df1f9345",
   "metadata": {
    "id": "3c217550-ac39-4cd8-9772-8775df1f9345"
   },
   "source": [
    "# Generating Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41700ae-dae2-4280-9207-2501b2857160",
   "metadata": {
    "id": "f41700ae-dae2-4280-9207-2501b2857160"
   },
   "source": [
    "You will create a small RNN network to learn how to write Shakespeare text letter by letter. Unfortunately these types of model take a very long time to train (hours) on a decent GPU so your results today in class won't be optimal. They may still impress you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6526346-1657-45f4-9861-52937754ed4d",
   "metadata": {
    "id": "f6526346-1657-45f4-9861-52937754ed4d"
   },
   "source": [
    "First load the dataset from the intenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d798096",
   "metadata": {
    "id": "2d798096"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8d2c0-e08e-4250-bea5-700ebb267a81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1741741743286,
     "user": {
      "displayName": "ARUSH BODLA",
      "userId": "02332828936023958936"
     },
     "user_tz": 240
    },
    "id": "73b8d2c0-e08e-4250-bea5-700ebb267a81",
    "outputId": "957162d5-1a8d-46cc-8bd1-50a5e4663585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Shakespeare text. Length: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Download the file\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Print some info\n",
    "print(\"Downloaded Shakespeare text. Length:\", len(text), \"characters\")\n",
    "print(text[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db65b4-5766-4920-80b5-fd6bbe4365a0",
   "metadata": {
    "id": "e0db65b4-5766-4920-80b5-fd6bbe4365a0"
   },
   "source": [
    "You need to transform this into an array of integers instead of characters. Use the sklearn LabelEncoder. You should find 64 distinct characters. To be sure, print out all the encoded integers and the character they correspond to. *If you want* you can lowercase all the letters first. This may speed up training some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b6a32-a662-4f2c-834f-e5d109784b7e",
   "metadata": {
    "id": "c46b6a32-a662-4f2c-834f-e5d109784b7e"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(list(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088c6f3-0c55-43ab-a4ca-c87bdcb8c96d",
   "metadata": {
    "id": "a088c6f3-0c55-43ab-a4ca-c87bdcb8c96d"
   },
   "source": [
    "Now as you did last class, convert this single array into X,y pairs, where each row of X is a string of characters and each y is the next character. For example\n",
    "\n",
    "'to be or not to b', 'e'\n",
    "'what light throug', 'h'\n",
    "\n",
    "You can choose how long you want the string of X chars to be (64,128,256 -- something in this range is reasonable. Smaller is faster to train. Longer makes a smarter model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f5dc8-de9d-4920-b72c-d5b311acbe90",
   "metadata": {
    "id": "f39f5dc8-de9d-4920-b72c-d5b311acbe90"
   },
   "outputs": [],
   "source": [
    "window_size = 64\n",
    "\n",
    "x = np.array([y[i - window_size:i] for i in range(window_size, len(y))])\n",
    "y = y[window_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6612a-d8c8-4c26-8bfa-881402cf1c1b",
   "metadata": {
    "id": "22b6612a-d8c8-4c26-8bfa-881402cf1c1b"
   },
   "source": [
    "Create a train/test set by choosing the first say 80% of the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48f791-c5e3-4eb1-bec8-8c9563cfe64e",
   "metadata": {
    "id": "ad48f791-c5e3-4eb1-bec8-8c9563cfe64e"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e32270-a155-46c1-9fed-6f70a0f4262f",
   "metadata": {
    "id": "06e32270-a155-46c1-9fed-6f70a0f4262f"
   },
   "source": [
    "Input to an RNN needs to be a 3D tensor. You will probably need to reshape your data.\n",
    "\n",
    "```python\n",
    "# Reshape the input data for LSTM (samples, timesteps, features)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "```\n",
    "\n",
    "For example if X_train.shape is (1000,100,1) then you have 1000 phrases each of length 100. The '1' wraps this in a 3D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c40cf-c090-41f5-a5e0-88d059f766cd",
   "metadata": {
    "id": "fa0c40cf-c090-41f5-a5e0-88d059f766cd"
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782aee1a-eb6b-482a-871d-a36ded7eef4e",
   "metadata": {
    "id": "782aee1a-eb6b-482a-871d-a36ded7eef4e"
   },
   "source": [
    "Define your RNN. Use one layer of RNN -- you can choose SimpleRNN, LSTM, or GRU with similar semantics. Here is an outline\n",
    "\n",
    "```python\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Input([None,1])\n",
    "model.add(GRU(128)) # 128 hidden units in one GRU layer\n",
    "model.add(Dense(alphabet_size, activation='softmax'))\n",
    "```\n",
    "\n",
    "The input is a sequence of *any length* (hence the `None`), but only 1D (characters). The output is a 1-hot encoded vectors over each character. Train this using cross entropy and adam optimizer. You can pick any batch size (larger is faster, consult the GPU memory usage). Don't expect super high accuracy, train only for a few epochs (10 or less, maybe much less! Start with 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce09d31-8331-4f2a-9070-eeea05c652b8",
   "metadata": {
    "id": "dce09d31-8331-4f2a-9070-eeea05c652b8"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input([None, 1]),\n",
    "    GRU(128),\n",
    "    Dense(encoder.classes_.shape[0], activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13465f",
   "metadata": {
    "id": "4f13465f"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['mse'])\n",
    "#history = model.fit(X_train, y_train, epochs=1, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7c5aa-89ae-482f-89a8-f64ce70577d5",
   "metadata": {
    "id": "88d7c5aa-89ae-482f-89a8-f64ce70577d5"
   },
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090ad16-bbd1-42aa-a359-afe60c3162a3",
   "metadata": {
    "id": "a090ad16-bbd1-42aa-a359-afe60c3162a3"
   },
   "source": [
    "This is a bit trickier than what we've done before. You need to process an input phrase, convert it to an array of ints, feed it to the model, get the logits of output, define a probability distribution,\n",
    "select an element according to that distribution, append the result to the input, and then do this over in a loop until you have generated as much output as you want. We can break this down into pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ba04a-3717-4a0e-879e-c8e49c8faff7",
   "metadata": {
    "id": "188ba04a-3717-4a0e-879e-c8e49c8faff7"
   },
   "source": [
    "First write `next_char(text, temp)` that gets the single next character predicted using `text` as input. Remember to employ the temperature. Here's a snippet that may help\n",
    "\n",
    "```python\n",
    "  probs = # output from your model\n",
    "  logits = np.log(probs)/temp # we have to invert the softmax to get back to logits, then divide by temp\n",
    "  char_id = tf.random.categorical(probs, num_samples=1) # helper function to apply softmax and then randomly sample\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c720a-f149-4a07-b06f-b4c4ca55de67",
   "metadata": {
    "id": "4e3c720a-f149-4a07-b06f-b4c4ca55de67"
   },
   "outputs": [],
   "source": [
    "def next_char(text, temp):\n",
    "    text = text.reshape(1, -1, 1)\n",
    "    probs = model.predict(text, verbose=0)\n",
    "    logits = np.log(probs) / temp\n",
    "    char_id = tf.random.categorical(logits, num_samples=1)\n",
    "    return char_id.numpy()[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e318e-216a-4bc4-80c2-1568dbc2042e",
   "metadata": {
    "id": "190e318e-216a-4bc4-80c2-1568dbc2042e"
   },
   "source": [
    "Now write `extend_text(text, n_chars, temp)` to add any number of characters to `text` by calling `next_char` repeatedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8ba3c-856d-4a5f-9443-7d3ad33ac6e3",
   "metadata": {
    "id": "2cd8ba3c-856d-4a5f-9443-7d3ad33ac6e3"
   },
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars, temp):\n",
    "    text_integers = encoder.transform(list(text))\n",
    "    text_integers = text_integers.reshape(-1, 1).tolist()\n",
    "    for _ in range(n_chars):\n",
    "        nc = next_char(np.array(text_integers[-window_size:]), temp)\n",
    "        text_integers.append([nc])\n",
    "        text += encoder.inverse_transform([nc])[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ec557",
   "metadata": {
    "id": "c41ec557"
   },
   "outputs": [],
   "source": [
    "teststr = \"To be or not \"\n",
    "out = extend_text(teststr, 4, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19wGy-NYnVPt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25725,
     "status": "ok",
     "timestamp": 1741804127015,
     "user": {
      "displayName": "ARUSH BODLA",
      "userId": "02332828936023958936"
     },
     "user_tz": 240
    },
    "id": "19wGy-NYnVPt",
    "outputId": "55c01589-d5bf-4bac-a4fb-c48d512bb9c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1753044d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1741741748015,
     "user": {
      "displayName": "ARUSH BODLA",
      "userId": "02332828936023958936"
     },
     "user_tz": 240
    },
    "id": "1753044d",
    "outputId": "ca74b33d-1d3c-4e55-8a2e-fd16f132b302"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'To be or not ;RWq'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb3f6c1-cc25-4e69-87bb-b9394b78d789",
   "metadata": {
    "id": "2fb3f6c1-cc25-4e69-87bb-b9394b78d789"
   },
   "source": [
    "Finally, generate some Shakespeare! Experiment with different seeds and seed lengths and temperatures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d62982-3067-48c6-b244-b2118b2f8aa1",
   "metadata": {
    "id": "e2d62982-3067-48c6-b244-b2118b2f8aa1"
   },
   "source": [
    "## Saving State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d436c-e180-4de6-855b-1c6518c92523",
   "metadata": {
    "id": "a16d436c-e180-4de6-855b-1c6518c92523"
   },
   "source": [
    "When training gets this involved you really need some good practices to save your work. Here's a callback that saves progress as you train. Especially important this is on Colab, which will stop and shutdown your session if you don't make it feel special all the time.\n",
    "\n",
    "```python\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint_filepath = 'best_shakespeare_model.keras'\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,  # Save the entire model\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    mode='min',  # Save the model when val_loss is minimized\n",
    "    save_best_only=True  # Only save the best model\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "history = model.fit(X_train, y_train, epochs=500,  validation_split=0.1, callbacks=[model_checkpoint_callback])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7f414-1ace-40b1-b842-289d2cdea7ba",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "49a7f414-1ace-40b1-b842-289d2cdea7ba",
    "outputId": "b774bf30-a623-45fa-95ac-4a1c7b0b5ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 6ms/step - accuracy: 0.2531 - loss: 2.6670 - val_accuracy: 0.3196 - val_loss: 2.3938\n",
      "Epoch 2/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 6ms/step - accuracy: 0.3232 - loss: 2.3752 - val_accuracy: 0.3389 - val_loss: 2.3148\n",
      "Epoch 3/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 6ms/step - accuracy: 0.3458 - loss: 2.2925 - val_accuracy: 0.3534 - val_loss: 2.2746\n",
      "Epoch 4/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 6ms/step - accuracy: 0.3566 - loss: 2.2471 - val_accuracy: 0.3547 - val_loss: 2.2603\n",
      "Epoch 5/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 6ms/step - accuracy: 0.3622 - loss: 2.2228 - val_accuracy: 0.3664 - val_loss: 2.2107\n",
      "Epoch 6/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 6ms/step - accuracy: 0.3687 - loss: 2.2012 - val_accuracy: 0.3649 - val_loss: 2.2204\n",
      "Epoch 7/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 6ms/step - accuracy: 0.3677 - loss: 2.2057 - val_accuracy: 0.3700 - val_loss: 2.1949\n",
      "Epoch 8/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 6ms/step - accuracy: 0.3751 - loss: 2.1826 - val_accuracy: 0.3684 - val_loss: 2.2059\n",
      "Epoch 9/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 6ms/step - accuracy: 0.3792 - loss: 2.1659 - val_accuracy: 0.3803 - val_loss: 2.1773\n",
      "Epoch 10/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 6ms/step - accuracy: 0.3817 - loss: 2.1584 - val_accuracy: 0.3832 - val_loss: 2.1562\n",
      "Epoch 11/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 6ms/step - accuracy: 0.3834 - loss: 2.1511 - val_accuracy: 0.3801 - val_loss: 2.1482\n",
      "Epoch 12/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 6ms/step - accuracy: 0.3862 - loss: 2.1394 - val_accuracy: 0.3854 - val_loss: 2.1407\n",
      "Epoch 13/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 6ms/step - accuracy: 0.3869 - loss: 2.1362 - val_accuracy: 0.3887 - val_loss: 2.1372\n",
      "Epoch 14/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 6ms/step - accuracy: 0.3894 - loss: 2.1286 - val_accuracy: 0.3868 - val_loss: 2.1469\n",
      "Epoch 15/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6ms/step - accuracy: 0.3912 - loss: 2.1211 - val_accuracy: 0.3924 - val_loss: 2.1221\n",
      "Epoch 16/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 6ms/step - accuracy: 0.3926 - loss: 2.1141 - val_accuracy: 0.3963 - val_loss: 2.1089\n",
      "Epoch 17/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 6ms/step - accuracy: 0.3955 - loss: 2.1068 - val_accuracy: 0.3898 - val_loss: 2.1168\n",
      "Epoch 18/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 6ms/step - accuracy: 0.3948 - loss: 2.1093 - val_accuracy: 0.3986 - val_loss: 2.0972\n",
      "Epoch 19/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 6ms/step - accuracy: 0.3967 - loss: 2.0993 - val_accuracy: 0.4042 - val_loss: 2.0842\n",
      "Epoch 20/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6ms/step - accuracy: 0.4000 - loss: 2.0873 - val_accuracy: 0.3995 - val_loss: 2.0910\n",
      "Epoch 21/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 6ms/step - accuracy: 0.3978 - loss: 2.0971 - val_accuracy: 0.3960 - val_loss: 2.1112\n",
      "Epoch 22/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 6ms/step - accuracy: 0.3995 - loss: 2.0886 - val_accuracy: 0.3933 - val_loss: 2.1170\n",
      "Epoch 23/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6ms/step - accuracy: 0.3988 - loss: 2.0907 - val_accuracy: 0.3936 - val_loss: 2.1085\n",
      "Epoch 24/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 6ms/step - accuracy: 0.3977 - loss: 2.0905 - val_accuracy: 0.3949 - val_loss: 2.1003\n",
      "Epoch 25/25\n",
      "\u001b[1m25095/25095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 6ms/step - accuracy: 0.3963 - loss: 2.0973 - val_accuracy: 0.3951 - val_loss: 2.1180\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint_filepath = 'best_shakespeare_model.keras'\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,  # Save the entire model\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    mode='min',  # Save the model when val_loss is minimized\n",
    "    save_best_only=True  # Only save the best model\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=25,  validation_split=0.1, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0nOP0AxR8o1j",
   "metadata": {
    "id": "0nOP0AxR8o1j"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extend_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m teststr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be, or not to b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mextend_text\u001b[49m(teststr, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(out)):\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m out[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m :\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extend_text' is not defined"
     ]
    }
   ],
   "source": [
    "teststr = \"To be, or not to b\"\n",
    "out = extend_text(teststr, 1000, 0.01)\n",
    "for i in range(len(out)):\n",
    "  if out[i] == '\\\\' :\n",
    "    print()\n",
    "    i += 1\n",
    "  else:\n",
    "    print(out[i], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bRQgekzR8rZA",
   "metadata": {
    "id": "bRQgekzR8rZA"
   },
   "outputs": [],
   "source": [
    "Interesting Samples:\n",
    "Double, double toil and trouble; Fire burn, and caldron bube\n",
    "I have so the soatt of the sorg of the sorg\n",
    "That the so the sorg of the sorg of the sorgs\n",
    "And that the sase of the sorg of the sorgs\n",
    "That the so the sorg of the sorg of the sorgs\n",
    "That the so the sand of the sorg of the sorgs\n",
    "That the so the sorg of the sorg of the sorgs\n",
    "That the so the sorg of the sorg of the sorgs\n",
    "That the so the sane of the sorg of the sorgs\n",
    "That the so the sorg of the sorg of the sorgs\n",
    "That the so the sand of the sorg of the sorgs\n",
    "That the so the sorg of the sorg of the sorgs\n",
    "That the so the sand of the sorg of the sorgs\n",
    "And that the sase of the sorg of the sorgs\n",
    "That the so the soatt of the sorg of the sorgs\n",
    "That the so the sorg of the sorg of the sorgs\n",
    "That the so the sorg of the sorg of the sorgs\n",
    "That the so the sane of the sorg of the sorgs\n",
    "That the so the sand of the sorg of the sorgs\n",
    "That the so the sand of the sorg of the sorgs\n",
    "That the so the sorg of the sorg of the sorgs\n",
    "That the so the sorg of the sorg of the sorgs\n",
    "That the so the sand of the sorg of the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiGKIIpjR8T",
   "metadata": {
    "id": "swiGKIIpjR8T"
   },
   "source": [
    "To be, or not to be sheer.\n",
    "\n",
    "LINGEOOE:\n",
    "Then the withengn of the partrer of the wither.\n",
    "\n",
    "QOMEO:\n",
    "Whll I will with the som  \n",
    "<br>\n",
    "\n",
    "To be, or not to bauuncn?\n",
    "\n",
    "CENVIENNO:\n",
    "My boote in and oyore bf ban ei wadr:\n",
    "That the pramt tn bro my berwr of wht,\n",
    "Fow the and pur withihrt the witr uhe drsger yas,\n",
    "Tuundit the oertltng's hang of Koldinghcr.\n",
    "\n",
    "NUCENTIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8tN2Fz0KjaJ9",
   "metadata": {
    "id": "8tN2Fz0KjaJ9"
   },
   "source": [
    "Double, double, toil and troubl!\n",
    "\n",
    "PUEEN MARGBREA:\n",
    "Au, I fave the wither from whth,\n",
    "The eatst ies be ieavg of wr be sear.\n",
    "\n",
    "COMIOOIO:\n",
    "Sav, the sei of yith wourgln forbrdmdns! Tiar put\n",
    "This psimgrt iis. Hi I the io the thpsle,\n",
    "Mo she"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DaZeeg3FjhHS",
   "metadata": {
    "id": "DaZeeg3FjhHS"
   },
   "source": [
    "Romeo, Romeo, wheteeng shen;  \n",
    "If my hor the dousents tn she poetters of weet  \n",
    "This pramcr of shan of hin but beltiee sosn.  \n",
    "N shat hese word ablenn thi your tayer,  \n",
    "Ou met I hear, ow oor shat comeer she nartrn,  \n",
    "Whir out t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2qXqxhwgjqaf",
   "metadata": {
    "id": "2qXqxhwgjqaf"
   },
   "source": [
    "sigma sigma on the walcr\n",
    "Thin thir heir in Porc, then I dank the senninr,\n",
    "Nrw be pnient; for the withier so then the maneent:\n",
    "I with the well of she porrpges of she comd;\n",
    "\n",
    "QECHTES:\n",
    "Inog, sir, and she kive! O, the gav!\n",
    "Yiat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g3JEiJzcoF1B",
   "metadata": {
    "id": "g3JEiJzcoF1B"
   },
   "source": [
    "My .keras:\n",
    "https://drive.google.com/file/d/1Fbnv8Xk0L6NXauTLN1e_YeMpopQlAlQ_/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NSosThxjn03d",
   "metadata": {
    "id": "NSosThxjn03d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
